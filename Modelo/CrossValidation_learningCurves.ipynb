{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fd6e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "import category_encoders as ce\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5227e462",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_cropped_2.csv')\n",
    "df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df.rename(columns={'1000':'age', '1001':'sex'},inplace=True)\n",
    "encoder = ce.BinaryEncoder()\n",
    "df['sex_0'] = encoder.fit_transform(df['sex'])['sex_0']\n",
    "df['sex_1'] = encoder.fit_transform(df['sex'])['sex_1']\n",
    "X = df.drop(['Target','sex','age','sex_0','sex_1'],axis=1)\n",
    "y = df.Target\n",
    "model = NuSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019ba9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdb3219",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6a6783",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0f509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcab840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df,x='Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a102e076",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df,x='sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fda7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df[df.sex != 'U'],x='sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a7ca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df,x='age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f2a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df[df.age != 999],x='age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56994c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(\n",
    "    estimator,\n",
    "    title,\n",
    "    X,\n",
    "    y,\n",
    "    axes=None,\n",
    "    ylim=None,\n",
    "    cv=None,\n",
    "    n_jobs=None,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator instance\n",
    "        An estimator instance implementing `fit` and `predict` methods which\n",
    "        will be cloned for each validation.\n",
    "\n",
    "    title : str\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Training vector, where ``n_samples`` is the number of samples and\n",
    "        ``n_features`` is the number of features.\n",
    "\n",
    "    y : array-like of shape (n_samples) or (n_samples, n_features)\n",
    "        Target relative to ``X`` for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array-like of shape (3,), default=None\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple of shape (2,), default=None\n",
    "        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, default=None\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, default=None\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like of shape (n_ticks,)\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the ``dtype`` is float, it is regarded\n",
    "        as a fraction of the maximum size of the training set (that is\n",
    "        determined by the selected validation method), i.e. it has to be within\n",
    "        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n",
    "        sets. Note that for classification the number of samples usually have\n",
    "        to be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "        estimator,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        train_sizes=train_sizes,\n",
    "        return_times=True,\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, \"o-\")\n",
    "    axes[1].fill_between(\n",
    "        train_sizes,\n",
    "        fit_times_mean - fit_times_std,\n",
    "        fit_times_mean + fit_times_std,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    fit_time_argsort = fit_times_mean.argsort()\n",
    "    fit_time_sorted = fit_times_mean[fit_time_argsort]\n",
    "    test_scores_mean_sorted = test_scores_mean[fit_time_argsort]\n",
    "    test_scores_std_sorted = test_scores_std[fit_time_argsort]\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_time_sorted, test_scores_mean_sorted, \"o-\")\n",
    "    axes[2].fill_between(\n",
    "        fit_time_sorted,\n",
    "        test_scores_mean_sorted - test_scores_std_sorted,\n",
    "        test_scores_mean_sorted + test_scores_std_sorted,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2839300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 4, figsize=(12, 17))\n",
    "\n",
    "\n",
    "\n",
    "title = \"Learning Curves (Nu SVC)\"\n",
    "# Cross validation with 50 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = StratifiedShuffleSplit(n_splits=30, test_size=0.15, random_state=0)\n",
    "\n",
    "estimator = NuSVC()\n",
    "plot_learning_curve(\n",
    "    estimator, title, X, y, axes=axes[:, 0], ylim=(0.3, 1.01), cv=cv, n_jobs=4\n",
    ")\n",
    "\n",
    "title = r\"(NearestCentroid)\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = StratifiedShuffleSplit(n_splits=30, test_size=0.15, random_state=0)\n",
    "estimator = NearestCentroid()\n",
    "plot_learning_curve(\n",
    "    estimator, title, X, y, axes=axes[:, 1], ylim=(0.3, 1.01), cv=cv, n_jobs=4\n",
    ")\n",
    "\n",
    "title = r\"(GradienBoosting)\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = StratifiedShuffleSplit(n_splits=30, test_size=0.15, random_state=0)\n",
    "estimator = GradientBoostingClassifier()\n",
    "plot_learning_curve(\n",
    "    estimator, title, X, y, axes=axes[:, 2], ylim=(0.3, 1.01), cv=cv, n_jobs=4\n",
    ")\n",
    "\n",
    "title = r\"(RandomForestClassifier)\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = StratifiedShuffleSplit(n_splits=30, test_size=0.15, random_state=0)\n",
    "estimator = RandomForestClassifier()\n",
    "plot_learning_curve(\n",
    "    estimator, title, X, y, axes=axes[:, 3], ylim=(0.3, 1.01), cv=cv, n_jobs=4\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ff6c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv('dataset_final_rotated.csv')\n",
    "df_2.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "#df_2.rename(columns={'1000':'age', '1001':'sex'},inplace=True)\n",
    "#encoder = ce.BinaryEncoder()\n",
    "#df_2['sex_0'] = encoder.fit_transform(df_2['sex'])['sex_0']\n",
    "#df_2['sex_1'] = encoder.fit_transform(df_2['sex'])['sex_1']\n",
    "X = df_2.drop(['Target'],axis=1)\n",
    "y = df_2.Target\n",
    "model = NuSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9127af84",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 4, figsize=(12, 17))\n",
    "\n",
    "\n",
    "\n",
    "title = \"Learning Curves (Nu SVC)\"\n",
    "# Cross validation with 50 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = StratifiedShuffleSplit(n_splits=30, test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = NuSVC()\n",
    "plot_learning_curve(\n",
    "    estimator, title, X, y, axes=axes[:, 0], ylim=(0.3, 1.01), cv=cv, n_jobs=4\n",
    ")\n",
    "\n",
    "title = r\"(NearestCentroid)\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = StratifiedShuffleSplit(n_splits=30, test_size=0.2, random_state=0)\n",
    "estimator = NearestCentroid()\n",
    "plot_learning_curve(\n",
    "    estimator, title, X, y, axes=axes[:, 1], ylim=(0.3, 1.01), cv=cv, n_jobs=4\n",
    ")\n",
    "\n",
    "title = r\"(GradientBoostingClassifier\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = StratifiedShuffleSplit(n_splits=30, test_size=0.2, random_state=0)\n",
    "estimator = GradientBoostingClassifier()\n",
    "plot_learning_curve(\n",
    "    estimator, title, X, y, axes=axes[:, 2], ylim=(0.3, 1.01), cv=cv, n_jobs=4\n",
    ")\n",
    "\n",
    "title = r\"(RandomForestClassifier)\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = StratifiedShuffleSplit(n_splits=30, test_size=0.2, random_state=0)\n",
    "estimator = RandomForestClassifier()\n",
    "plot_learning_curve(\n",
    "    estimator, title, X, y, axes=axes[:, 3], ylim=(0.3, 1.01), cv=cv, n_jobs=4\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a32da35",
   "metadata": {},
   "source": [
    "- uncensored dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6b89e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_unCens.csv')\n",
    "df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df.rename(columns={'1000':'age', '1001':'sex'},inplace=True)\n",
    "encoder = ce.BinaryEncoder()\n",
    "df['sex_0'] = encoder.fit_transform(df['sex'])['sex_0']\n",
    "df['sex_1'] = encoder.fit_transform(df['sex'])['sex_1']\n",
    "X = df.drop(['Target','age','sex_0','sex_1','sex'],axis=1)\n",
    "y = df.Target\n",
    "model = NuSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7043684e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 4, figsize=(12, 17))\n",
    "\n",
    "\n",
    "\n",
    "title = \"Learning Curves (Nu SVC)\"\n",
    "# Cross validation with 50 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = StratifiedShuffleSplit(n_splits=15, test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = NuSVC()\n",
    "plot_learning_curve(\n",
    "    estimator, title, X, y, axes=axes[:, 0], ylim=(0.3, 1.01), cv=cv, n_jobs=4\n",
    ")\n",
    "\n",
    "title = r\"(NearestCentroid)\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = StratifiedShuffleSplit(n_splits=15, test_size=0.2, random_state=0)\n",
    "estimator = NearestCentroid()\n",
    "plot_learning_curve(\n",
    "    estimator, title, X, y, axes=axes[:, 1], ylim=(0.3, 1.01), cv=cv, n_jobs=4\n",
    ")\n",
    "\n",
    "title = r\"(GradientBoostingClassifier)\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = StratifiedShuffleSplit(n_splits=15, test_size=0.2, random_state=0)\n",
    "estimator = GradientBoostingClassifier()\n",
    "plot_learning_curve(\n",
    "    estimator, title, X, y, axes=axes[:, 2], ylim=(0.3, 1.01), cv=cv, n_jobs=4\n",
    ")\n",
    "\n",
    "title = r\"(RandomForestClassifier)\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = StratifiedShuffleSplit(n_splits=15, test_size=0.2, random_state=0)\n",
    "estimator = RandomForestClassifier()\n",
    "plot_learning_curve(\n",
    "    estimator, title, X, y, axes=axes[:, 3], ylim=(0.3, 1.01), cv=cv, n_jobs=4\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1e02df",
   "metadata": {},
   "source": [
    "- uncensored VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa43c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_rotated_unCens_vgg19.csv')\n",
    "df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df.rename(columns={'1000':'age', '1001':'sex'},inplace=True)\n",
    "encoder = ce.BinaryEncoder()\n",
    "df['sex_0'] = encoder.fit_transform(df['sex'])['sex_0']\n",
    "df['sex_1'] = encoder.fit_transform(df['sex'])['sex_1']\n",
    "X = df.drop(['Target','age','sex_0','sex_1','sex'],axis=1)\n",
    "y = df.Target\n",
    "model = NuSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27f31cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 4, figsize=(12, 17))\n",
    "\n",
    "\n",
    "\n",
    "title = \"Learning Curves (Nu SVC)\"\n",
    "# Cross validation with 50 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = StratifiedShuffleSplit(n_splits=15, test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = NuSVC()\n",
    "plot_learning_curve(\n",
    "    estimator, title, X, y, axes=axes[:, 0], ylim=(0.3, 1.01), cv=cv, n_jobs=4\n",
    ")\n",
    "\n",
    "title = r\"(NearestCentroid)\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = StratifiedShuffleSplit(n_splits=15, test_size=0.2, random_state=0)\n",
    "estimator = NearestCentroid()\n",
    "plot_learning_curve(\n",
    "    estimator, title, X, y, axes=axes[:, 1], ylim=(0.3, 1.01), cv=cv, n_jobs=4\n",
    ")\n",
    "\n",
    "title = r\"(GradientBoostingClassifier)\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = StratifiedShuffleSplit(n_splits=15, test_size=0.2, random_state=0)\n",
    "estimator = GradientBoostingClassifier()\n",
    "plot_learning_curve(\n",
    "    estimator, title, X, y, axes=axes[:, 2], ylim=(0.3, 1.01), cv=cv, n_jobs=4\n",
    ")\n",
    "\n",
    "title = r\"(RandomForestClassifier)\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = StratifiedShuffleSplit(n_splits=15, test_size=0.2, random_state=0)\n",
    "estimator = RandomForestClassifier()\n",
    "plot_learning_curve(\n",
    "    estimator, title, X, y, axes=axes[:, 3], ylim=(0.3, 1.01), cv=cv, n_jobs=4\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fcc407",
   "metadata": {},
   "outputs": [],
   "source": [
    "htgf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c552846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a9bf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    NuSVC(), \n",
    "    NearestCentroid(), \n",
    "    GaussianNB(),\n",
    "    RandomForestClassifier(),\n",
    "    ExtraTreesClassifier()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edc75ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(GaussianNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae30cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter grids for the various classifiers\n",
    "NuSVC_parameters = {\n",
    "    'classifier__nu' : [0.3,0.5,0.7],\n",
    "    'classifier__kernel' : ['linear','poly','rbg','sigmoid'],\n",
    "    'classifier__gamma':['scale','auto']\n",
    "}\n",
    "NearestCentroid_parameters = {\n",
    "    'classifier__shrink_threshold' : [None]\n",
    "    \n",
    "}\n",
    "GaussianNB_parameters = {\n",
    "    'classifier__var_smoothing': [1e-9,1e-8,1e-7]\n",
    "    \n",
    "}\n",
    "RandomForest_parameters = {\n",
    "    'classifier__n_estimators': [50,100,150,200],\n",
    "    'classifier__criterion' : ['gini','entropy']\n",
    "}\n",
    "ExtraTree_parameters = {\n",
    "    'classifier__n_estimators': [10,100,1000],\n",
    "    'classifier__criterion' : ['gini','entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9ed601",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    NuSVC_parameters,\n",
    "    NearestCentroid_parameters,\n",
    "    GaussianNB_parameters,\n",
    "    RandomForest_parameters,\n",
    "    ExtraTree_parameters\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c508d0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = []\n",
    "# iterate through each classifier and use GridSearchCV\n",
    "for i, classifier in enumerate(classifiers):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,shuffle=True)\n",
    "    # create a Pipeline object\n",
    "    print(f'I\"m in cycle {i}')\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    clf = GridSearchCV(pipe,              # model\n",
    "              param_grid = parameters[i], # hyperparameters\n",
    "              #scoring='accuracy',         # metric for scoring\n",
    "              cv=30)                      # number of folds\n",
    "    clf.fit(X, y)\n",
    "    print(\"Tuned Hyperparameters :\", clf.best_params_)\n",
    "    print(\"Accuracy :\", clf.best_score_)\n",
    "    # add the clf to the estimators list\n",
    "    estimators.append((classifier.__class__.__name__, clf))\n",
    "    #clf.fit(X_5_train, y_5_train)\n",
    "    #print(\"Tuned Hyperparameters :\", clf.best_params_)\n",
    "    #print(\"Accuracy :\", clf.best_score_)\n",
    "    # add the clf to the estimators list\n",
    "    estimators.append((classifier.__class__.__name__, clf))\n",
    "print(estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f69ebc",
   "metadata": {},
   "source": [
    "- only male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf78f7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m = df[df.sex == 'M']\n",
    "X = df_m.drop(['Target','age','sex_0','sex_1','sex'],axis=1)\n",
    "y = df_m.Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b67b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = []\n",
    "# iterate through each classifier and use GridSearchCV\n",
    "for i, classifier in enumerate(classifiers):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,shuffle=True)\n",
    "    # create a Pipeline object\n",
    "    print(f'I\"m in cycle {i}')\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    clf = GridSearchCV(pipe,              # model\n",
    "              param_grid = parameters[i], # hyperparameters\n",
    "              #scoring='accuracy',         # metric for scoring\n",
    "              cv=30)                      # number of folds\n",
    "    clf.fit(X, y)\n",
    "    print(\"Tuned Hyperparameters :\", clf.best_params_)\n",
    "    print(\"Accuracy :\", clf.best_score_)\n",
    "    # add the clf to the estimators list\n",
    "    estimators.append((classifier.__class__.__name__, clf))\n",
    "    #clf.fit(X_5_train, y_5_train)\n",
    "    #print(\"Tuned Hyperparameters :\", clf.best_params_)\n",
    "    #print(\"Accuracy :\", clf.best_score_)\n",
    "    # add the clf to the estimators list\n",
    "    estimators.append((classifier.__class__.__name__, clf))\n",
    "print(estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821d21a2",
   "metadata": {},
   "source": [
    "- only female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ef2fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m = df[df.sex == 'F']\n",
    "X = df_m.drop(['Target','age','sex_0','sex_1','sex'],axis=1)\n",
    "y = df_m.Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d2cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = []\n",
    "# iterate through each classifier and use GridSearchCV\n",
    "for i, classifier in enumerate(classifiers):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,shuffle=True)\n",
    "    # create a Pipeline object\n",
    "    print(f'I\"m in cycle {i}')\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    clf = GridSearchCV(pipe,              # model\n",
    "              param_grid = parameters[i], # hyperparameters\n",
    "              #scoring='accuracy',         # metric for scoring\n",
    "              cv=30)                      # number of folds\n",
    "    clf.fit(X, y)\n",
    "    print(\"Tuned Hyperparameters :\", clf.best_params_)\n",
    "    print(\"Accuracy :\", clf.best_score_)\n",
    "    # add the clf to the estimators list\n",
    "    estimators.append((classifier.__class__.__name__, clf))\n",
    "    #clf.fit(X_5_train, y_5_train)\n",
    "    #print(\"Tuned Hyperparameters :\", clf.best_params_)\n",
    "    #print(\"Accuracy :\", clf.best_score_)\n",
    "    # add the clf to the estimators list\n",
    "    estimators.append((classifier.__class__.__name__, clf))\n",
    "print(estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a33ed9d",
   "metadata": {},
   "source": [
    "- male with age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184d1bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m = df[df.sex == 'M']\n",
    "X = df_m.drop(['Target','sex_0','sex_1','sex'],axis=1)\n",
    "y = df_m.Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NuSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6c53a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = []\n",
    "# iterate through each classifier and use GridSearchCV\n",
    "for i, classifier in enumerate(classifiers):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,shuffle=True)\n",
    "    # create a Pipeline object\n",
    "    print(f'I\"m in cycle {i}')\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    clf = GridSearchCV(pipe,              # model\n",
    "              param_grid = parameters[i], # hyperparameters\n",
    "              #scoring='accuracy',         # metric for scoring\n",
    "              cv=15)                      # number of folds\n",
    "    clf.fit(X, y)\n",
    "    print(\"Tuned Hyperparameters :\", clf.best_params_)\n",
    "    print(\"Accuracy :\", clf.best_score_)\n",
    "    # add the clf to the estimators list\n",
    "    estimators.append((classifier.__class__.__name__, clf))\n",
    "    #clf.fit(X_5_train, y_5_train)\n",
    "    #print(\"Tuned Hyperparameters :\", clf.best_params_)\n",
    "    #print(\"Accuracy :\", clf.best_score_)\n",
    "    # add the clf to the estimators list\n",
    "    estimators.append((classifier.__class__.__name__, clf))\n",
    "print(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e92e26c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
